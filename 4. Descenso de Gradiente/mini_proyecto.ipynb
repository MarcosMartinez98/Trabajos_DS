{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION -  MINIPROYECTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMA 1 - GRADIENT DESCENT\n",
    "\n",
    "El **descenso de gradiente** es un algoritmo de optimización que se utiliza para ajustar los parámetros de un modelo de regresión lineal. El objetivo es encontrar los valores de los parámetros que minimizan la función de costo, que mide la diferencia entre las predicciones del modelo y los valores reales.\n",
    "\n",
    "El descenso de gradiente funciona iterativamente, actualizando los valores de los parámetros en cada iteración para reducir la función de costo. En cada iteración, se calcula el gradiente de la función de costo con respecto a los parámetros y se actualizan los parámetros en la dirección opuesta al gradiente. La tasa de aprendizaje controla el tamaño de los pasos que se dan en cada iteración.\n",
    "\n",
    "Para aplicar el descenso de gradiente a un modelo de regresión lineal, se utiliza el **MSE** como función de costo. El MSE mide la diferencia entre las predicciones del modelo y los valores reales al cuadrado. El objetivo es minimizar el MSE ajustando los valores de los parámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso a paso del algoritmo de descenso de gradiente para optimizar un modelo de regresión lineal:\n",
    "\n",
    "1. Inicializa los parámetros del modelo con valores aleatorios.\n",
    "2. Calcula la función de costo utilizando los valores actuales de los parámetros.\n",
    "3. Calcula el gradiente de la función de costo con respecto a los parámetros.\n",
    "4. Actualiza los parámetros en la dirección opuesta al gradiente multiplicado por la tasa de aprendizaje.\n",
    "5. Repite los pasos 2-4 hasta que la función de costo converja a un mínimo o decidas parar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMO LO TENEIS QUE HACER VOSOTROS:\n",
    "\n",
    "1. Inicializa los parametros `m` y `c` a cero o con valores aleatorios.\n",
    "2. Elegimos un numero de iteraciones (eg. 100) y un ``learning_rate`` (eg. 0.01)\n",
    "3. Inicializamos valores para X e Y. X que sean numpy arrays de 10 o 20 numeros (eg. range(10)) y la Y pueden ser parecida a la X pero con numeros arriba o abajo para simular aletoriedad (eg. 2,2,4,2,5,7,6,9,8,10)\n",
    "4. Arrancamos el bucle for.\n",
    "5. Calculamos la derivada del error respecto de `m` y `c`:\n",
    "    - `dm = 2/n*sum((y-(m*x+c))*(-x))`\n",
    "    - `dc = 2/n*sum((y-(m*x+c))*(-1))`\n",
    "6. Actualizamos los parametros: `m=m-dm*lr` y `c=c-dc*lr`\n",
    "7. Cada iteracion haceis un plot de la linea `x` y `m*x+c` respecto a los puntos originales (plot de x e y) y os guardais el las imagenes (Si haceis 100 iteraciones, tendreis 100 imagenes. Podeis valorar solo guardar por ejemplo cada 5 iteraciones por no hacer tantos plots).\n",
    "8. Una vez terminamos printeamos m y c y con los plots generados teneis quer buscar alguna libreria en python y montar un gif con las imagenes de los plots:\n",
    "\n",
    "![gif](https://th.bing.com/th/id/R.79e22f97090c346d704a68f7151e8cda?rik=oJV36GZyA1otdA&riu=http%3a%2f%2fcdn-images-1.medium.com%2fmax%2f640%2f1*eeIvlwkMNG1wSmj3FR6M2g.gif&ehk=0NUalJOl26VxY8ndNrkpV7GwYM1NVtJ5kMxU6jm5jB0%3d&risl=&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA**: Todos los valores son orientativos. Podeis modificar los datos como querais. De hecho, os animo a que probeis y trasteeis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e2afa",
   "metadata": {},
   "source": [
    "Tomaremos el modelo más simple, con una feature y un target.\n",
    "Vamos a aplicar el descenso de gradiente para calcular los parámetros m y c de nuestro modelo de regresión lineal. Para ello partimos de la la función de coste MSE:\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a0fef",
   "metadata": {},
   "source": [
    "Si nuestro modelo de regresión lineal es $\\hat{y}_i = m \\cdot x_i + c$, entonces:\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (m \\cdot x_i + c - y_i)^2$$\n",
    "\n",
    "Derivando parcialmente con respecto a m y c el MSE obtenemos el gradiente de la función:\n",
    "$$\n",
    "\\text{Las derivadas parciales son:} \\\\\n",
    "\\frac{\\partial MSE}{\\partial m} = \\frac{2}{n} \\sum_{i=1}^{n} x_i (m \\cdot x_i + c - y_i) \\\\\n",
    "\\\\\n",
    "\\frac{\\partial MSE}{\\partial c} = \\frac{2}{n} \\sum_{i=1}^{n} (m \\cdot x_i + c - y_i)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Las reglas de actualización para el descenso de gradiente son:} \\\\\n",
    "\n",
    "m_{\\text{nuevo}} = m_{\\text{actual}} - \\alpha \\cdot \\left( \\frac{2}{n} \\sum_{i=1}^{n} x_i (m_{\\text{actual}} \\cdot x_i + c_{\\text{actual}} - y_i) \\right) \\\\\n",
    "\\\\\n",
    "c_{\\text{nuevo}} = c_{\\text{actual}} - \\alpha \\cdot \\left( \\frac{2}{n} \\sum_{i=1}^{n} (m_{\\text{actual}} \\cdot x_i + c_{\\text{actual}} - y_i) \\right)\n",
    "$$\n",
    "\n",
    "Donde $\\alpha$ es el *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c0455",
   "metadata": {},
   "source": [
    "Vamos ahora con el código.\n",
    "Tomaremos 30 valores 'aleatorios' para x e y.\n",
    "Establecemos para la primera iteración m = 0 y c = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5d01f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Descenso de Gradiente con 200 iteraciones y LR=0.0001\n",
      "\n",
      "Descenso de Gradiente Finalizado.\n",
      "Valores finales: m = 30.1800, c = -148.6738\n"
     ]
    }
   ],
   "source": [
    "# comando magico para mostrar la grafica movil en una ventana\n",
    "%matplotlib qt\n",
    "\n",
    "# librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "\n",
    "x = np.arange(1,31)\n",
    "y = (np.arange(1,31))**2\n",
    "\n",
    "# Inicializar parámetros para el Descenso de Gradiente\n",
    "m = 0      \n",
    "c = -150   \n",
    "learn_rate = 0.0001 # tomo lr pequeño para que se vea bien como se ajusta la recta\n",
    "n_iteraciones = 200 \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ion() # Modo Interactivo\n",
    "\n",
    "# Calcular los límites del gráfico una vez para que los ejes no se \"zoomeen\" y \"deszoomeen\"\n",
    "x_min, x_max = 0,35\n",
    "y_min, y_max = -200,920\n",
    "\n",
    "\n",
    "print(f\"Iniciando Descenso de Gradiente con {n_iteraciones} iteraciones y LR={learn_rate}\")\n",
    "\n",
    "\n",
    "for i in range(n_iteraciones): # Descenso de gradiente, actualizamos m y c y sus parciales (que se han de avaluar en cada iteracion)\n",
    "    parcial_m = (2/len(x))*sum(x*(m*x + c - y))\n",
    "    parcial_c = (2/len(x))*sum((m*x + c - y))  \n",
    "    m = m - learn_rate * parcial_m\n",
    "    c = c - learn_rate * parcial_c\n",
    "\n",
    "    \n",
    "    if i % 10 == 0 or i == n_iteraciones - 1: # Se actualiza cada 10 iteraciones y en la última\n",
    "        plt.clf() # Limpiar la figura actual \n",
    "\n",
    "        # Volver a dibujar el scatter plot de los datos originales\n",
    "        plt.scatter(x, y, label='Datos \"Aleatorios\"', color='blue', alpha=0.7)\n",
    "\n",
    "        # Dibujar la recta de regresión con los parámetros m y c actuales\n",
    "        x_line = np.linspace(0, 35, 100) # Rango de X para la línea\n",
    "        y_line = m * x_line + c\n",
    "        plt.plot(x_line, y_line, color='red', linestyle='--', linewidth=2, label='Recta de Regresión')\n",
    "\n",
    "        # Configurar etiquetas, título y límites de los ejes para el fotograma actual\n",
    "        plt.title(f'Iteración {i+1}/{n_iteraciones}\\nm = {m:.4f}, c = {c:.4f},MSE = {1/len(x)*sum((m*x + c - y)**2)}')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Target')\n",
    "        plt.grid(True, linestyle=':', alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.xlim(x_min, x_max) \n",
    "        plt.ylim(y_min, y_max) \n",
    "\n",
    "        plt.draw() # Forzar el redibujado de la figura\n",
    "        plt.pause(0.1) # Pausa para que se vea como va cambiando\n",
    "\n",
    "print(f\"\\nDescenso de Gradiente Finalizado.\")\n",
    "print(f\"Valores finales: m = {m:.4f}, c = {c:.4f}\")\n",
    "\n",
    "plt.ioff() # Desactivar el modo interactivo al finalizar\n",
    "plt.show() # Mostrar la gráfica final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando animación con 80 fotogramas. Esto puede tardar un poco...\n",
      "\n",
      "¡Animación guardada con éxito como 'descenso_gradiente_lineal.gif'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marcos\\miniconda3\\envs\\data_analysis_env\\Lib\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marcos\\miniconda3\\envs\\data_analysis_env\\Lib\\site-packages\\matplotlib\\animation.py\", line 932, in _start\n",
      "    self.event_source.add_callback(self._step)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'add_callback'\n"
     ]
    }
   ],
   "source": [
    "# Crear Gif\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation # Importamos FuncAnimation\n",
    "\n",
    "# --- 1. Datos X e Y ---\n",
    "x = np.arange(1, 31)\n",
    "y = (np.arange(1, 31))**2\n",
    "\n",
    "# --- 2. Parámetros del Descenso de Gradiente ---\n",
    "# Usamos nombres con '_current' para distinguirlos de los parámetros\n",
    "# que se actualizarán globalmente dentro de la función 'update'\n",
    "m_current = 0\n",
    "c_current = -150\n",
    "learn_rate = 0.0001\n",
    "n_iteraciones = 80\n",
    "\n",
    "# --- 3. Configuración de la Gráfica para la Animación ---\n",
    "# Creamos la figura y los ejes usando plt.subplots()\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Configuramos los límites de los ejes una sola vez\n",
    "x_min, x_max = 0, 35\n",
    "y_min, y_max = -200, 920\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "# Dibujamos el scatter plot de los datos originales (permanece estático)\n",
    "ax.scatter(x, y, label='Datos \"Aleatorios\"', color='blue', alpha=0.7)\n",
    "\n",
    "# Creamos la línea de regresión inicial (esta será la que se actualizará en cada fotograma)\n",
    "# Usamos una coma después de 'line' para desempaquetar la tupla que devuelve ax.plot\n",
    "line, = ax.plot([], [], color='red', linestyle='--', linewidth=2, label='Recta de Regresión')\n",
    "\n",
    "# Configuramos las etiquetas y la leyenda\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Target')\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# --- 4. Función de Actualización para FuncAnimation ---\n",
    "# Esta función se llamará para cada fotograma de la animación\n",
    "def update(frame):\n",
    "    global m_current, c_current # Accedemos y modificamos las variables globales\n",
    "\n",
    "    # Calcular la predicción actual para el modelo LINEAL (m*x + c)\n",
    "    y_predicha = m_current * x + c_current\n",
    "    error = y_predicha - y\n",
    "\n",
    "    # Calcular las derivadas parciales\n",
    "    parcial_m = (2 / len(x)) * np.sum(x * error)\n",
    "    parcial_c = (2 / len(x)) * np.sum(error)\n",
    "\n",
    "    # Actualizar los parámetros (paso de Descenso de Gradiente)\n",
    "    m_current = m_current - learn_rate * parcial_m\n",
    "    c_current = c_current - learn_rate * parcial_c\n",
    "\n",
    "    # Calcular el MSE actual para mostrarlo en el título\n",
    "    mse = (1 / len(x)) * np.sum(error**2)\n",
    "\n",
    "    # Generar los puntos para la nueva recta de regresión\n",
    "    x_line = np.linspace(x_min, x_max, 100)\n",
    "    y_line = m_current * x_line + c_current\n",
    "\n",
    "    # Actualizar los datos de la línea en la gráfica\n",
    "    line.set_data(x_line, y_line)\n",
    "\n",
    "    # Actualizar el título con la iteración actual y los valores de los parámetros y MSE\n",
    "    ax.set_title(f'Iteración {frame+1}/{n_iteraciones}\\nm = {m_current:.4f}, c = {c_current:.4f}, MSE = {mse:.4f}')\n",
    "\n",
    "    # Devuelve los objetos que han sido modificados (para optimización de redibujo)\n",
    "    return line, ax.title # Devolver la línea y el título para que blit funcione correctamente\n",
    "\n",
    "\n",
    "# --- 5. Crear la Animación ---\n",
    "print(f\"Creando animación con {n_iteraciones} fotogramas. Esto puede tardar un poco...\")\n",
    "# FuncAnimation(figura, función_update, número_de_fotogramas, tiempo_entre_fotogramas_ms, blit)\n",
    "ani = FuncAnimation(fig, update, frames=n_iteraciones, interval=50, blit=True, repeat=False) # interval=50ms -> 20 FPS\n",
    "\n",
    "\n",
    "# --- 6. Guardar la Animación como GIF ---\n",
    "try:\n",
    "    # writer='pillow' es el escritor más común para GIFs si tienes Pillow instalado\n",
    "    # fps (frames per second) controla la velocidad del GIF\n",
    "    ani.save('descenso_gradiente_lineal.gif', writer='pillow', fps=20)\n",
    "    print(\"\\n¡Animación guardada con éxito como 'descenso_gradiente_lineal.gif'!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError al guardar el GIF: {e}\")\n",
    "    print(\"Asegúrate de tener Pillow instalado (pip install Pillow).\")\n",
    "    print(\"Si el error persiste y Pillow está instalado, prueba con blit=False en FuncAnimation.\")\n",
    "\n",
    "# Cierra la figura para evitar que se muestre una ventana de Qt no deseada al final del script\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad52bed",
   "metadata": {},
   "source": [
    "### Descenso de gradiente para un polinomio de grado 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be62ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Descenso de Gradiente con 1500 iteraciones y LR=1e-08\n",
      "\n",
      "Descenso de Gradiente Finalizado.\n",
      "Valores finales:a= 0.9933, m = 0.0407, c = 0.0018\n"
     ]
    }
   ],
   "source": [
    "# comando magico para mostrar la grafica movil en una ventana\n",
    "%matplotlib qt\n",
    "\n",
    "# librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "\n",
    "x = np.arange(1,31)\n",
    "y = (np.arange(1,31))**2\n",
    "\n",
    "# Inicializar parámetros para el Descenso de Gradiente\n",
    "a = 0\n",
    "m = 0      \n",
    "c = 0      \n",
    "learn_rate = 0.00000001 # tomo lr pequeño para que se vea bien como se ajusta la recta\n",
    "n_iteraciones = 1500\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ion() # Modo Interactivo\n",
    "\n",
    "# Calcular los límites del gráfico una vez para que los ejes no se \"zoomeen\" y \"deszoomeen\"\n",
    "x_min, x_max = 0,35\n",
    "y_min, y_max = 0,920\n",
    "\n",
    "\n",
    "print(f\"Iniciando Descenso de Gradiente con {n_iteraciones} iteraciones y LR={learn_rate}\")\n",
    "\n",
    "\n",
    "for i in range(n_iteraciones): # Descenso de gradiente, actualizamos a,m y c y sus parciales (que se han de avaluar en cada iteracion)\n",
    "    parcial_a = (2/len(x))*sum((x**2)*(a*x**2 + m*x + c - y))\n",
    "    parcial_m = (2/len(x))*sum(x*(a*x**2 + m*x + c - y))\n",
    "    parcial_c = (2/len(x))*sum((a*x**2 + m*x + c - y))  \n",
    "    a = a - learn_rate * parcial_a\n",
    "    m = m - learn_rate * parcial_m\n",
    "    c = c - learn_rate * parcial_c\n",
    "\n",
    "    \n",
    "    if i % 50 == 0 or i == n_iteraciones - 1: # Se actualiza cada 10 iteraciones y en la última\n",
    "        plt.clf() # Limpiar la figura actual \n",
    "\n",
    "        # Volver a dibujar el scatter plot de los datos originales\n",
    "        plt.scatter(x, y, label='Datos \"Aleatorios\"', color='blue', alpha=0.7)\n",
    "\n",
    "        # Dibujar la recta de regresión con los parámetros m y c actuales\n",
    "        x_line = np.linspace(0, 35, 100) # Rango de X para la línea\n",
    "        y_line = a*(x_line**2) + m * x_line + c\n",
    "        plt.plot(x_line, y_line, color='red', linestyle='--', linewidth=2, label='Polinomio de Regresión')\n",
    "\n",
    "        # Configurar etiquetas, título y límites de los ejes para el fotograma actual\n",
    "        plt.title(f'Iteración {i+1}/{n_iteraciones}\\na= {a:.4f},m = {m:.4f}, c = {c:.4f}')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Target')\n",
    "        plt.grid(True, linestyle=':', alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.xlim(x_min, x_max) \n",
    "        plt.ylim(y_min, y_max) \n",
    "\n",
    "        plt.draw() # Forzar el redibujado de la figura\n",
    "        plt.pause(0.002) # Pausa para que se vea como va cambiando\n",
    "\n",
    "print(f\"\\nDescenso de Gradiente Finalizado.\")\n",
    "print(f\"Valores finales:a= {a:.4f}, m = {m:.4f}, c = {c:.4f}\")\n",
    "\n",
    "plt.ioff() # Desactivar el modo interactivo al finalizar\n",
    "plt.show() # Mostrar la gráfica final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando animación con 120 fotogramas. Esto puede tardar un poco...\n",
      "\n",
      "¡Animación guardada con éxito como 'descenso_gradiente_polinomial.gif'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marcos\\miniconda3\\envs\\data_analysis_env\\Lib\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marcos\\miniconda3\\envs\\data_analysis_env\\Lib\\site-packages\\matplotlib\\animation.py\", line 932, in _start\n",
      "    self.event_source.add_callback(self._step)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'add_callback'\n"
     ]
    }
   ],
   "source": [
    "# crear Gif\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation # Importamos FuncAnimation\n",
    " \n",
    "# --- 1. Datos X e Y ---\n",
    "x = np.arange(1, 31)\n",
    "y = (np.arange(1, 31))**2\n",
    "\n",
    "# --- 2. Parámetros del Descenso de Gradiente ---\n",
    "# Usamos nombres con '_current' para distinguirlos de los parámetros\n",
    "# que se actualizarán globalmente dentro de la función 'update'\n",
    "a_current = 0\n",
    "m_current = 0\n",
    "c_current = 0\n",
    "learn_rate = 0.0000001 # Tasa de aprendizaje ajustada para este modelo\n",
    "n_iteraciones = 120 # Número de iteraciones\n",
    "\n",
    "# --- 3. Configuración de la Gráfica para la Animación ---\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Configuramos los límites de los ejes una sola vez\n",
    "x_min, x_max = 0, 35\n",
    "y_min, y_max = 0, 920\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "# Dibujamos el scatter plot de los datos originales (permanece estático)\n",
    "ax.scatter(x, y, label='Datos \"Aleatorios\"', color='blue', alpha=0.7)\n",
    "\n",
    "# Creamos la línea de regresión inicial (esta será la que se actualizará en cada fotograma)\n",
    "# Usamos una coma después de 'curve' para desempaquetar la tupla que devuelve ax.plot\n",
    "curve, = ax.plot([], [], color='red', linestyle='-', linewidth=2, label='Polinomio de Regresión')\n",
    "\n",
    "# Configuramos las etiquetas y la leyenda\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Target')\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# --- 4. Función de Actualización para FuncAnimation ---\n",
    "# Esta función se llamará para cada fotograma de la animación\n",
    "def update(frame):\n",
    "    global a_current, m_current, c_current # Accedemos y modificamos las variables globales\n",
    "\n",
    "    # Calcular la predicción actual para el modelo POLINOMIAL (a*x**2 + m*x + c)\n",
    "    y_predicha = a_current * (x**2) + m_current * x + c_current\n",
    "    error = y_predicha - y\n",
    "\n",
    "    # Calcular las derivadas parciales\n",
    "    parcial_a = (2 / len(x)) * np.sum((x**2) * error)\n",
    "    parcial_m = (2 / len(x)) * np.sum(x * error)\n",
    "    parcial_c = (2 / len(x)) * np.sum(error)\n",
    "\n",
    "    # Actualizar los parámetros (paso de Descenso de Gradiente)\n",
    "    a_current = a_current - learn_rate * parcial_a\n",
    "    m_current = m_current - learn_rate * parcial_m\n",
    "    c_current = c_current - learn_rate * parcial_c\n",
    "\n",
    "    # Generar los puntos para la nueva curva de regresión\n",
    "    x_curve = np.linspace(x_min, x_max, 100)\n",
    "    y_curve = a_current * (x_curve**2) + m_current * x_curve + c_current\n",
    "\n",
    "    # Actualizar los datos de la curva en la gráfica\n",
    "    curve.set_data(x_curve, y_curve)\n",
    "\n",
    "    # Actualizar el título con la iteración actual y los valores de los parámetros\n",
    "    ax.set_title(f'Iteración {frame+1}/{n_iteraciones}\\na= {a_current:.6f}, m = {m_current:.6f}, c = {c_current:.6f}') # Más decimales para 'a'\n",
    "\n",
    "    # Devuelve los objetos que han sido modificados (para optimización de redibujo)\n",
    "    return curve, ax.title # Devolver la curva y el título para que blit funcione correctamente\n",
    "\n",
    "\n",
    "# --- 5. Crear la Animación ---\n",
    "print(f\"Creando animación con {n_iteraciones} fotogramas. Esto puede tardar un poco...\")\n",
    "# FuncAnimation(figura, función_update, número_de_fotogramas, tiempo_entre_fotogramas_ms, blit)\n",
    "ani = FuncAnimation(fig, update, frames=n_iteraciones, interval=50, blit=True, repeat=False) # interval=50ms -> 20 FPS\n",
    "\n",
    "\n",
    "# --- 6. Guardar la Animación como GIF ---\n",
    "try:\n",
    "    # writer='pillow' es el escritor más común para GIFs si tienes Pillow instalado\n",
    "    # fps (frames per second) controla la velocidad del GIF\n",
    "    ani.save('descenso_gradiente_polinomial.gif', writer='pillow', fps=20)\n",
    "    print(\"\\n¡Animación guardada con éxito como 'descenso_gradiente_polinomial.gif'!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError al guardar el GIF: {e}\")\n",
    "    print(\"Asegúrate de tener Pillow instalado (pip install Pillow).\")\n",
    "    print(\"Si el error persiste y Pillow está instalado, prueba con blit=False en FuncAnimation.\")\n",
    "\n",
    "# Cierra la figura para evitar que se muestre una ventana de Qt no deseada al final del script\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMA 2 - ECUACION NORMAL\n",
    "\n",
    "El modelo de Linear Regression es unico en todo el universo ML. Es el unico que se puede resolver de forma analitica resolviendo una ecuacion sin necesidad de optimizar usando por ejemplo el descenso de gradiente. Para resolverlo se usa una ecuacion que se llama ECUACION NORMAL.\n",
    "\n",
    "La ecuación normal es una forma de resolver la regresión lineal sin utilizar el descenso de gradiente. En lugar de iterar para encontrar los valores de los parámetros que minimizan la función de costo, la ecuación normal calcula los valores de los parámetros directamente.\n",
    "La ecuación normal para la regresión lineal simple es:\n",
    "\n",
    "$$\\theta=(X^{T}X)^{−1}X^{T}y$$\n",
    "Donde θ es el vector de parámetros, X es la matriz de características, y es el vector de valores objetivo y −1 denota la inversa de una matriz.\n",
    "La ecuación normal se deriva al igualar el gradiente de la función de costo a cero. La solución resultante es la que minimiza la función de costo.\n",
    "\n",
    "AQUI TENEIS LA FORMULA RESUELTA: https://interactivechaos.com/es/manual/tutorial-de-machine-learning/la-ecuacion-normal\n",
    "\n",
    "Buscadla y aplicarla con vuestros datos de `X` e ``y``, y os calcula directamente los parametros ``m`` y ``c``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0620264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normal = x.reshape(30,-1)\n",
    "y_normal = x_normal**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c98f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.c_[np.ones((30, 1)), x_normal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd565fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto = -165.3333333333332\n",
      "Coeficiente = 30.999999999999993\n"
     ]
    }
   ],
   "source": [
    "best_W = np.linalg.inv(X_1.T.dot(X_1)).dot(X_1.T).dot(y_normal)\n",
    "print(f'Intercepto = {best_W[0][0]}\\nCoeficiente = {best_W[1][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMA 3 - SKLEARN\n",
    "\n",
    "Todos sabeis que si entrenamos un modelo de linear regression con sklearn podemos obtener los coeficientes con el metodo `.coef_` y el intercepto con `.intercept`.\n",
    "\n",
    "Haz un fit del modelo con tus datos X e y, y comprueba que valores de coeficiente (m) y que valor de intercepto (c) te da\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a5065483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y,x,columns=['target']).reset_index()\n",
    "df = df.rename(columns={'index': 'feature'}, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5eabde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto = 30.394002068252327\n",
      "Coeficiente = -150.32669769045157\n",
      "MSE:3882.6363561902804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df.loc[:,['feature']]\n",
    "Y = df.loc[:,'target']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "lr.predict(X_test)\n",
    "print(f'Intercepto = {lr.coef_[0]}\\nCoeficiente = {lr.intercept_}')\n",
    "print(f'MSE:{mean_squared_error(lr.predict(X_test),Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401973ea",
   "metadata": {},
   "source": [
    "# FORMA 4 - Ecuación normal para número de columnas igual a uno  \n",
    "Sabemos que el coeficiente m y el intercepto c vienen dados por:\n",
    "$m = \\frac{Cov(x,y)}{Var(x)}$ y $c = \\bar{y} - m\\cdot \\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e1f492db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto = 32.06896551724138\n",
      "Coeficiente = -181.9022988505747\n"
     ]
    }
   ],
   "source": [
    "\n",
    "covar_xy = np.cov(x,y)[0,1]\n",
    "var_x = np.var(x)\n",
    "m4 = covar_xy/var_x\n",
    "c4 = np.mean(y) - m4*np.mean(x)\n",
    "print(f'Intercepto = {m4}\\nCoeficiente = {c4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacdc8c",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85117040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(\n",
    "    {\n",
    "        'Parámetro':['m','c'],\n",
    "        'Descenso de Gradiente':[30.1916,-148.9091],\n",
    "        'Ecuación Normal':[round(best_W[1][0],4), round(best_W[0][0],4)],\n",
    "        'Sklearn':[round(lr.coef_[0],4),round(lr.intercept_)],\n",
    "        'Ecuación normal 1 feature':[round(m4,4),round(c4,4)]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c146a2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parámetro</th>\n",
       "      <th>Descenso de Gradiente</th>\n",
       "      <th>Ecuación Normal</th>\n",
       "      <th>Sklearn</th>\n",
       "      <th>Ecuación normal 1 feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m</td>\n",
       "      <td>30.1916</td>\n",
       "      <td>31.0000</td>\n",
       "      <td>30.394</td>\n",
       "      <td>32.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>-148.9091</td>\n",
       "      <td>-165.3333</td>\n",
       "      <td>-150.000</td>\n",
       "      <td>-181.9023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parámetro  Descenso de Gradiente  Ecuación Normal  Sklearn  \\\n",
       "0         m                30.1916          31.0000   30.394   \n",
       "1         c              -148.9091        -165.3333 -150.000   \n",
       "\n",
       "   Ecuación normal 1 feature  \n",
       "0                    32.0690  \n",
       "1                  -181.9023  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
